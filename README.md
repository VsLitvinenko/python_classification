# python_classification
Т.к. датасеты слишком большого размера для репозитория, их можно докачать отдельно по ссылке ----> https://disk.yandex.ru/d/9pS-9ovHgoXdQg и вынести папку "dataset" в корень проекта.

P.s. при обработке текста (filter_data_text.py) нет необходимости использовать и стемминг, и лемматизацию (70-75 строки). Достаточно чего-то одного, стемминг менее точен, однако проходит НАМНОГО быстрее, что при среднем уровне терпения позволяет создать выборку намного бОльшего объема, чем используя лемматизацию, что компенсирует потерю точности при токенизации.
